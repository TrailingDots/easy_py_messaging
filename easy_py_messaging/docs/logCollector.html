<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
    <head>
        <title>Easy Log Collection: Distributed Logs for Raspberry Pi</title>
        <script src="./javascript/toc.js" type="text/javascript"></script>
        <link rel="stylesheet" href="css/article.css" type="text/css" />
    </head>

    <body onload="generateTOC(document.getElementById('toc'));">

        <h1>Easy Log Collection by Messaging</h1>

        <div id="toc"></div>


        <h2>Benefits of Easy Log Messaging</h2>

        <center>
        <div style="width:300px;border:1px solid #cecece; padding:10px;">
            <blockquote>It turns out that bulding reusable messaging systems
            is really difficult, which is why few free and open source
            (FOSS) projects ever get tried, and why commercial
            messaging products are complex, inflexible and brittle. ...
            It takes weeks to learn to use it, and months to create
            stable architectures that don't crash when things get
            hairy.</blockquote>
            <div class="credit" style="text-align:center;">
                <cite><a href="http://zguide.zeromq.org/page:all#Why-We-Needed-ZeroMQ">Pieter Hintjens in ZeroMQ</a></cite>
            </div>
        </div>
        </center>

        <p>This package strives to provide a simple, useful package for
        small to medium projects. The learning curve should require only a
        minimal amount of python coding to produce acceptable results.</p>

        <p>This log messaging package targets implementors of the
        Raspberry Pi, Arduino, etc. boards: a name server and a control
        service. The possiblity that noobies can manager multiple
        nodes envisions a future of connected computers with an
        ever widening wealth of applications.</p> 

        <p>By writing a wrapper on the industry standard ZeroMQ messaging
        library, logging gains significant enhancements from the existing
        python logging module. The ZeroMQ messaging system works well within
        a single process, between mulitple processes within a single system
        and, most importantly, with distributed nodes over a network.</p>

        <p>This log collection code provides a solution to centralizing logs
        from multiple nodes.  With the inevitable rise of distributed systems
        everywhere, a centralized log offers an in-depth view for problem
        solving.</p>

        <p>Logs have been around as long as computers and form the
        core of many distributed and real-time applications. Logs
        form <i>the</i> way to debug live systems. Without some form
        of logging, how can anyone gain insight into significant
        problems?</p>

        <p>Distributed computing relies on logs to maintain a useful view of
        both hardware and software. As a cross cutting concern, logging
        remains along with security a necessity for modern systems.</p>
        
        <h2>Specifically Targeted: Logging Raspberry Pi Systems</h2>

        <p>As the price of credit credit card sized computers and their
        associated instrumentation continues to fall, distributed systems
        will continue to proliferate.  Connecting these systems together
        raises problems of debugging and tracking. Logging supplies a
        significant contribution towards a viable solution.</p>
        
        <p>Once a Raspberry Pi system has been installed and starts working,
        new and different ideas frequently require both software and hardware
        upgrades. Perhaps some of these upgrades would be easier with the
        addition of another Raspberry Pi system? This could hold especially
        true for those creations involving hardware expansion. The existing
        system works fine, but upgrades threaten the stability of the
        existing system. After all, you've likely expended a significant
        effort, haven't you?</p>

        <p>Consider an additional Raspberry Pi system for those enhancements.
        As even more ideas pop up, could a convenient way to expand involve
        adding even more Raspberry Pi systems? With the cost of hardware
        dropping dramatically, how much effort would you care to devote to
        integrating those changes in software? If you are like most people,
        upgrading software can represent a serious investment of both time
        and money. How much easier would a new Raspberry Pi board
        represent?<p>

        <p>Suppose your home automation system has been growing. You started
        with the interior of your house. Now you want to expand to your
        backyard and perhaps even to your front yard. Are three system on the
        horizon? One for the interior, one for the front and one for the
        back?  What about one system to rule them all? Do you relish running
        all that new wiring when those boards could use wifi instead? Or have
        you become so device crazy that the GPIO cannot handle all those
        inputs and you <i>must</i> use additional nodes?</p>

        <p>Another example would be that hydroponics system that has started
        to function well. You've spent a fair amount of time and now you want
        another system with new ideas implemented from lessons on the
        original prototype.  The existing system works fine, so changing it
        has little appeal: "If it ain't broke, don't fix it!" The new system,
        however, can absorb all that new hardware and software that's been
        bubbling though your mind.</p>

        <h2>Still Using Print Statements?</h2>

        <p>Many noobies seem satisfied with the ubiquitous "print"
        statements.  Their console output becomes cluttered with tons of
        disorganized, unfiltered and opaque output.</p>

        <p>The lowly "print" statements cannot work outside the narrow
        confines of brief ad-hoc debugging. Print statements for debugging
        clutter both code and console output. They cannot become a permanent
        feature of your application. Definitely a better solution must
        exist.</p>


        <h2>Logging within a Single System</h2>

        <p>This log messaging solution extends beautifully to multiple
        systems.  However, a single system can definitely use this logging
        system to great advantage. Starting with a single system with logging
        this can greatly simplify expansion to multi-node systems. That
        single system can become a wealth of information for further
        implementations.</p>

        <p>This easy log messaging solution can even live as a thread within
        a single process! The degree of flexibility offered along with ease of
        implementation allows this solution to shine!</p>

        <h2>Using the Easy Log Messaging Application</h2>

        <p>The easy log messaging application creates a consolidated view for
        tracking what happens in your multi-node systems. It provides a
        central logging system that collects logs from one or more
        applications and stores them into either a simple text file or a
        database. This messaging speed has been demonstrated to be adequate
        for all but the most demanding applications.</p>

        <p>The logging API has been simplified so that even noobies should
        not have trouble in using the code. Only one IP address and one port
        need be specified.</p> 

        <p>The process generating the logs can reside on the same node as the
        logger.  Alternatively, the process creating the logs can run on a
        system removed from the logging system.</p>

        <p>A separate logging process presents a much stronger resource than
        as a thread of the application task. If the application were to crash,
        the last few logs may be buffered and lost in the crash.  If a
        separate logging task were used, it could receive those last few logs
        and post them to your logging repository.</p>

        <p>This local vs. remote flexibility opens the possibility of storing
        logs on a desktop in your home while monitoring nodes in your
        garden, backyard, garage and other places of interest. The remote
        systems could crash and the logging system will assist in debugging
        that crash. Your crashed system may or may not have information
        necessary to fixing that crash. The log file, however, could
        supply valuable information relevant to that crash.</p>

        <p>As the various parts of the easy log messaging system are
        presented, necessary debugging techniques will also be suggested.
        When I am learning a new system and attempting to follow the steps for
        installing or implementing that system, I notice that the authors
        usually ignore problems. They just assume everything will work.  In
        these situations I am always wondering, "How do I verify that this is
        correct?" And, when I make a mistake, how do I recover?</p>

        <h2>The Primary Goal</h2>

        <p>This easy log messaging application offers a logging server
        capable of collecting logs from multiple sources and storing them into
        a text file or database system. The initial release saves only to a
        text file.</p>

        <p>The primary goal supplies a useful and <i>easy</i> to use logging
        system that will work well with supplied enhancements for controlling
        and monitoring distributed Raspberry Pis, Arduinos or similar systems.
        This package provides general purpose software capable of providing
        solid logging capabilitiies for almost any system supporting python
        and ZeroMQ!</p>

        <p>Our working example model is a hydroponics system. The hydroponics
        system has 2 Raspberry Pis controlling various devices and sends logs
        to a desktop system.  The desktop may send control commands. Various
        logs from both systems get sent to the logger to monitor and track
        events in these systems.</p>


        <h1>Introductory Matters</h1>

        <h2>Python as an Implementation Language</h2>

        <p>Because Python has demonstrated global acceptance for many projects
        on our tageted boards, the easy log messaging system will continue
        this tradition. Python provides an easy-to-code implementation
        language as well as convenient debugger support. This project combines
        the python power with ZeroMQ to create a powerful logging system.</p>

        <h2>Audience</h2>

        <p>We assume the reader has one or more systems such as Raspberry Pi,
        Arduino or BeagleBoard. Additionally, a desktop system running Debian
        or Ubuntu or any linux system will ease development because desktop
        systems typically have greater capablities such as disk space, memory,
        speed, and multiple monitors. Because Raspberry Pi boards typically
        use Debian, a Debian desktop minimizes system interface problems.</p>

        <p>One or more Raspberry Pi nodes is <i>not</i> a requirement. This
        system works perfectly well with processes on a single desktop.</p>
        
        <p>Developing an application using a single Raspberry Pi system also
        works well.</p>

        <p>Some python knowledge is assumed. Intermediate to advanced noobies
        can use this code while absolute beginners will likely struggle.
        Certainly advanced programmers should expect only minor issues.</p>

        <h2>Original Hardware Development</h2>

        <p>This code was originally developed on a Linux Centos 7 system.
        It was tested on Raspberry Pi B 2 boards running the Raspian Debian OS
        with 8Gb SSD and 512Mb RAM. The testing between boards used the
        USB wireless ports on the Raspberry Pi hardware. Frequently
        the Raspberry Pi boards communicated with the Centos 7 system as a
        logging collection server.</p>

        <p>A MacBookPro with OSX El-Capitan also validated this code.
        Noobies must be aware that XCode must be installed as a
        pre-requisite.</p>

        <h2>Getting the Code</h2>

        <p>The application source code resides at: <a
            href="https://github.com/TrailingDots/easy_py_messaging.git">easy_py_messaging</a>.
        The code should work on any recent linux system and will work on a
        recent Raspberry Pi Raspian OS. The Raspberry Pi Logger has been built
        on a Raspian OS, OSX El-Capitan and a Centos 7 Linux system. Unbuntu
        and Debian OSs should allow for an easy build. No attempt has been
        made for Windows versions.</p>


        <h1>Installation</h1>

        <p>Each of the following commands may require a <code>sudo</code> in
        front of the command to allow root to install in secure locations.
        Prefix the commands with <code>sudo</code> if a "permissions denied"
        message appears.</p>

        <p>If the results of a <code>pip</code> command results in
        "Requirements already satisfied...", upgrade to the latest version by
        supplying the <code>--upgrade</code> switch as in: <code>pip install
        --upgrade cython</code> .

        <ul>

            <li>Install the <a href="http://zeromq.org/area:download#toc5">ZMQ
                C libraries</a>.  Alternatively, a download of binaries should
            work equally well. <a href="zeromq.org">ZeroMQ</a> has detailed
            procedures for installation if necessary. <b>Hint</b>: If you run
            into troubles with a message related to <code>sodium</code> in
            either the configure or make steps, find the line in configure that
            reads "<code>have_sodium_library="yes"</code> and change
            <code>"yes"</code> to <code>"no"</code></li>

            <li>Install <a
            href="https://github.com/zeromq/pyzmq.git">PyZMQ</a>,
            the python bindings to ZeroMQ.</li>

        </ul>

        <p>ZeroMQ should exist and python code should work with ZeroMQ,
        Test this by:</p>

        <pre >
        python
        import zmq</pre>

        <p>If errors appear, please refer to the web for workarounds.</p>

        <p>Continue to install the easy_py_messaging code:</p>

        <p><code>cd</code> to a work directory.</p>

        <p>Install the easy log messagng application: <br/>
        <code class="indent">git clone
            https://github.com/TrailingDots/easy_py_messaging.git</code><br/>
        If you get "permission denied", then you need to prefix the
        previous command with <code>sudo</code> to install with root
        access.</p>

        <p>Move into the just-downloaded code:<br/>
        <code class=indent>cd
            easy_py_messaging</code></p>

        <p>Finish the installation with a final install:<br/><code class=indent>python
            setup.py install</code><br/>Again, for "permission denied" error,
        prefix with "<code>sudo</code>"</p>
        
        <p>Ensure the logCollector has been installed:<br/>
        <code class=indent>which
            logCollector</code><br/>If you get an error containing something
        like "<code>no logCollector in ...</code>", refer to the troubleshooting
        section.</p>

        <p>Start the log Collector: <br/>
        <code class=indent>logCollector
            --log-file=/tmp/xyz.log</code><br/>
        This will create logs in /tmp/xyz.log.</p>

        <p>In another terminal window, send a log: <br/>
        <code class=indent>logCmd Simple
            test</code></p>

        <p>Examine the log in /tmp/xyz.log:<br/> 
        <code class=indent>cat /tmp/xyz.log</code><br/>
        <code class=indent>2016-04-14T13:49:19.851
            INFO	Simple test,host=brass</code><br/> The timestamp reflects
        only the time this log was received and the host signifies your current
        hostname.</p>

        <p>Congratulations! You have a working system!</p>

        <h1>Structure of Log Entries</h1>

        <p>A single log as implemented by this application consists of three
        fields separated by a tab:</p>

        <pre>
        timestamp \t level \t payload
        </pre>

        <p>Each log entry saved in the text file has the following
        tab sepatated field:</p>
            <table>
                <tr>
                    <td><b>Name</b></td>
                    <td><b>Meaning</b></td>
                </tr>
                <tr>
                    <td>timestamp</td>
                    <td>The timestamp of arrival of the log entry as
                        determined by logCollector. Using the timestamp
                        of logCollector provides a more accurate time
                        than attempting to synchronize multiple nodes
                        to the same time. Timestamps always display
                        themselves with an ISO8601 format. More on
                        ISO8601 later.</td>
                </tr>
                <tr>
                    <td>Level</td>
                    <td>The level of the log entry. Levels tag an entry
                        to a level of importance. The current levels are,
                        from least important to most:<br/>
                        <ul>
                            <li>DEBUG - a debugging message</li>
                            <li>INFO - an informational message</li>
                            <li>CMD - records a command.</li>
                            <li>WARNING - a warning that something is amiss.</li>
                            <li>ERROR - something has gone wrong that should not happen.</li>
                            <li>CRITICAL - something very wrong has happened
                            to the system. Likely a crash or something that
                            prevents system operation.</li>

                        </ul>

                    </td>

                </tr>

                <tr>
                    <td>Payload</td>

                    <td>A payload may consist of absolutely anything. However,
                        a highly recommended structure would be <code>keyword=value</code>
                        separated by commas. An example:<br/><code
                        class=indent> pump01=ON,waterLevel=normal</code><br/>
                    This allows easy parsing in processing log entries.  Each
                    payload gets terminated with a &lt;host=hostname&gt; by
                    the logging code. An example of the displayed payload:
                    <br/><code class=indent>
                    pump01=ON,waterLevel=normal,host=brass</code><br/>
                    This example was run with the logCollector on a system
                    named <code>brass</code>. A user should not
                    add their own host field.  This host field supplies an
                    identification for the source for each log entry.</td>
                </tr>

            </table>

        <p>Using this suggested format greatly simplifies processing
        log files by other programs. Large log files become unwieldy
        to read. Do youself a favor and implement a uniform logging
        philosophy.</p>

        <p>Notice that the suggested format allows the payload
        to trivially convert to JSON format. JSON format blends
        in well with many modern utilities.</p>

        <h1>Components</h1>

        <p>The log collection application uses multiple components
        to offer rich to logging capabilities.</p>

        <p>A brief note on configuration files. All configuration files
        load by reading the contents that get evaluated by the python
        interpreter using <code>eval()</code>. This can represent a
        security threat! If this is a concern, please contact the author
        to bump up security enhancements.</p>

            <h2>logCollector - Collecting and saving log messages</h2>

            <p>The process that collects logs sent from other threads,
            processes or nodes. Logs currently get saved in simple text
            files.</p>

            <pre>
logCollector --help
logCollector [--log-file=logFilename] [port=&lt;port#&gt;] [-a] [-t]
     [--noisy] [--config=&lt;config-filename&gt;]

     --log-file=logFilename = name of file to place logs
         The default file if logs.log in the current dir
     --port=&lt;port#&gt; = port to listen for incoming logs
     --noisy  = Logs echo to stdout. Normall not echoed.
     -a  Logs will be appended to logFilename. Default
     -t  logFilename will be truncated before writing logs.

-a and -t apply only when --file specifics a valid filename.
--noisy or -n : Echo message to console. Useful for debugging.

If logFilename does not exist, it will be created.
If logFilename does exist, by default, logs get appended.

logCollector --help
logCollector -h
     This message

To toggle "noisy" printing of messages received:
    kill -USR1 &lt;pid&gt;

To cycle through levels of messages retained:
    kill -USR2 &lt;pid&gt;
logCollector: exiting:Exiting logCollector
            </pre>

            <ul>

            <li><code>-log-file=&lt;filename&gt;</code> - The filename where
            logs will get saved. Using an absolute filename provides assurity
            that the files get stored in a well known place.  Caveats such as
            an existing directory and permissions apply, of course.</li>

            <li><code>-a</code> - New logs should normally get appended to the
            end of any existing logs. This is the default.</li>

            <li><code>-t</code> - An existing log-file gets truncated and all
            new log entries will replace the existing log file. Sometimes
            useful when debugging a system.</li>

            <li><code>--port=&lt;port#&gt;</code> - A port number in which
            the log collector listens for incoming logs. Applications sending
            logs for collection must use this port.</li>

            <li><code>--help</code> - The help display.</li>

            <li><code>--noisy</code> - The collection of logs get echoed
            to the console. This provides a convenient way to monitor
            log progress during development. The <code>--noisy</code>
            switch gets toggled by:<br/><code class="indent">kill -USR1 &lt;pid&gt;</code><br/>
            and the resulting "noisy" status gets printed on the console.</li>



            </ul>

            <h3>Configuration file</h3>

            <p>In normal operations the logCollector begins by reading a
            configuration file from <code>.logcollectorrc</code> in the same
            directoryt logCollector began executing. If that file does not
            exist, it tries to read <code>$HOME/.logcollectorrc</code>, that
            is, the configuration file from the user's home directory.</p>

            <p>Users may, of course, supply a configuration filename on
            the command line with the <code>--config=&lt;filename&gt;</code>
            argument.</p>

            <p>If the configuration file does not exist, logs get saved in the
            invocation directory with the name of <code>logs.log</code>.</p>

            <p>The configuration file eases the implementation of more
            complex systems by storing any special setting necessary.</p>

            <p>The default contents of <code>.logcollectorrc</code>:</p>

            <pre>
{
    "append":   True,
    "out_file": './logs.log',
    "noisy":    False,
    "port":     5570,
}
            </pre>

            <ul>

                <li>append - True to append new logs to the tail of
                any existing logs. This satisfies most normal requirements.</li>

                <li>out_file - The name of the output log file. This should typically 
                use an absolute filename. Any relative filename will
                write logs relative to the directory where logCollector
                was started.</li>

                <li>noisy - Logs do not normally get echoed to the console.
                However, while debugging a system, listing all incoming logs
                to the console as well as to the out_file provides a
                convenient monitoring scheme.</li>

                <li>port - The port number that logCollector listens
                for incoming logs.</li>

            </ul>

            <p>The <code>noisy</code> config option map also get toggled from
            the command line as required.  When logCollector starts, it
            displays the process id, pid, of itself. In the help message
            above this was 18083.  To toggle console display of log
            messages, send the command<br/><code class="indent">kill -USR1
                18083</code><br/>This has been found useful in
            debugging a system. The only trick has been in remembering the pid
            of logCollector.</p>


            <h2 id="logFilterApp">logFilterApp - Assistance in log interpretation</h2>

            <p>This command line utility program converts from the the
            standard log format to either JSON or CSV. JSON remains the
            preferred format.</p>

            <p>Given a repository of logs, what is the difficulty of
            discovering problems in logs of several thousand entries? How
            would you handle that mind numbing experience? Now repeat that
            same experience over and over when necessity dictates various
            queries of those logs. Now consider that enterprise systems
            typically contains log files of million or even billions of log
            entries?</p>

            <p>Various log analysis tools exist to select and interpret log
            files. This initial release contains an elementary analysis tool
            that can output logs in either CSV (Comma Separated Values) or JSON
            (JavaScript Object Notation).  CSV conveniently loads into
            almost any spreadsheet. JSON offers ease of loading into
            most modern programming languages: python, NodeJS, JavaScript,
            etc.</p>

            <pre>
logFilterApp [--out-file=outfile] [--in-file=infile]
    [--start=&lt;ISO8601 start date&gt;] 
    [--end=&gt;iso8601 end date&gt;]
    [--JSON | --CSV] 
    [--help]
    [--level=&lt;level name&gt;] 
    [--out-file=output file] # output goes here. Default: stdout
    [--in-file=input file]   # input file here. Default: stdin

--start=&lt;iso8601 start date&gt;     # start date iso formatted
--end=&lt;iso8601   end   date&gt;     # end   date iso formatted
 If start with no end, continue to present time.
--level=LEVEL   # Handle only from LEVEL up.
 DEBUG,CMD,INFO,WARNING,ERROR,CRITICAL are the levels.
--JSON          # output format is JSON (default)
--CSV           # Output format is CSV
--help          # This message
            </pre>

            <ul>

                <li><code>--out-file=filename</code> - The name of an
                output file to place the results. Output files format
                themselves as either JSON or CSV. The console is the
                default output.</li>

                <li><code>--in-file=log_filename</code> - The name of
                the logfile to use as input. The console is the default.</li>

                <li><code>--start=&lt;ISO8601 start date&gt;</code> - An ISO8601
                formatted date to begin filtering log data. See the section
                below on ISO8601 dates.</li>

                <li><code>--end=&lt;ISO8601 end date&gt;</code> - An ISO8601
                formatted date to stop filtering log data. See the section
                below on ISO8601 dates.</li>

                <li><code>--JSON</code> - The output format is JSON format.
                JSON provides an easily read coding format
                for almost all languages. JSON is the default format.</li>

                <li><code>--CSV</code> - The output format is CSV, Comma
                Separated Variables. Speadsheets can easily consume
                this information. CAREFUL: The log files must have
                been carefully formatted to intelligently produce
                CSV. JSON is the default format.</li>

                <li><code>--level=&lt;level&gt;</code> - Only log entries
                at or above this level get processes. The default
                level is WARNING, meaning only WARNING, ERROR and
                CRITICAL log entries appear in the output.</li>

                <li><code>--help</code> - The help message.</li>

            </ul>
                

        <h3>A note on start and end dates</h3>

        <p>The default start and end dates are from the start of
        the log file to the end of the log file. This default
        gets used when no dates are specified. For larger log
        files this may yield abnormally large output files.</p>

        <p>Supplying a start date results in ignoring log
        entries before the specified date. The remainder of the log
        file gets processed. A common use would be reading today's
        logs.</p>

        <p>Supplying an end date with no start date results in reading from
        the beginning of the log file up to and including the end date.</p>

        <p>Supplying both a start and end date will, of course,
        read all logs between these two dates. Gathering information
        on last weeks operations could be a typical case.</p>

        <p>An end date occurring before the start date results in
        an error message.</p>

            
        <h3>Configuration Files</h3>

        <p>Filtering log entries can result in powerful analytics
        for your system. Depending upon your needs, various
        scenarios present themselves over and over.</p>

        <p>Perhaps the solution to handle these repetitious queries
        represents itself in a configuration file.  Using multiple
        configuration files allows easy invocation of multiple queries.</p>

        <p>The logFilterApp configuration file contains a python
        dictionary of matching keyword=values the same as
        described in the help output above. Due to "standard"
        linux nameing conventions, the configation file
        uses '_' instead of '-'.</p>

        <p>A sample configuration file to read every log entry
        for all times with a level of <code>CMD</code> and above:</p>

        <pre>
cat $HOME/cmd.conf  # List the config file.
{
'in_file': 'data/base.data',
'level': 'CMD',
}
        </pre>

        <p>Notice the absense of start and end times. The default
        start time means the start of the Linux epoch. The default
        end time means now. Thus, the entire contents of the
        file will filtered with a level of WARNING.</p>

        <p>But what if all  you need is a single day's worth of
        logs? When a configuration file gets loaded, further
        command line arguments override existing arguments. For
        example, if you were working on your taxes Apr 15,2016,
        you could use this configuration file to read data only for
        Apr 15, 2016:</p>

        <pre>
logFilterApp --config=$HOME/cmd.conf --start=2016-04-15T00:00:00.000 --end=2016-05-15T23:59:59.000
        </pre>

        <h3>Supplying your own log filters</h3>

        <p>The current filtering allows powerful results. But what if
        your requirements were more extensive? What if you needed to
        search for an <code>ERROR</code> and process related logs?
        The current filtering does not allow easy management of such
        requests.</p>

        <p>Or, you might have an interest in only one node, or only a
        few specific devices. Only your immagination limit the number of queries .</p>

        <p>For now, start with simply filtering all the logs.
        Write code that handles your choice of JSON or CSV data.</p>

        <p>Later additions will offer better filtering options.</p>


        <h2 id="logCmd">logCmd - Easy logging for shell commands</h2>

        <p>logCmd fills the need to log information from the
        linux shell. Either the console or shell scripts may
        send logs to the logCollector.</p>

        <p>Using logCmd is simplicity itself:<br/><code class=indent>
            logCmd This is an important message</code><br/>
        The string "This is an important message" get appended
        to the existing logs.<p>

        <p>When using logCmd pay attention to the format. The
        command line arguments get sent as the are written.
        If the arguments can be free-form, meaning literally
        anything, then just start typing.</p>

        <p>If the arguments contain data that may be processed later by
        something like logFilterApp, take care to write a proper parsable
        string. For instance, suppose a shell script reads a
        temperature. This could be sent as:<br/><code
        class=indent>logCmd therm01=74.6</code></p>

        <p><b>WARNING!</b> logCmd will hang unless the logCollector
        process is not running. logCmd does not attempt to determine
        if a logCollector is running because multiple logCollectors
        may be run on different ports. Also, the code for this
        utility has been deliverately left as simple as possible.<p>

        <p>See an explanation of logCmd and its coding as
        a simple example of using easy log messaging in
        your own code.</p>

        <p><b>Exercise:</b> Add a timeout to logCmd is the logCollector
        is not running. Appropriate error messages should also
        record this event. Don't forget proper setting of return
        codes.</p>

        <h2 id="loggingSpeedTest">loggingSpeedTest - How fast can your system go?</h2>

        <p>We all like to know how fast our system can process data. This quick
        and dirty utility send 100,000 messages to the logCollector and reports
        on the timing.</p>

        <p>Due to the volume of messages, start the <code>logCollector</code>
        with an output to <code>/dev/null</code> to discard logging output:<br/>
        <code class="indent">logCollector --log-file=/dev/null</code><br/></p>

        <p>Do not rely on a single timing, but run multiple times and get an
        average:</p>

        <pre>
for ndx in $(seq 10)
&gt; do
&gt;    python loggingSpeedTest.py
&gt; done
100000 logs, elapsed time: 1.387242
72085 messages per second
100000 logs, elapsed time: 1.339986
74627 messages per second
100000 logs, elapsed time: 1.356970
73693 messages per second
100000 logs, elapsed time: 1.364641
73279 messages per second
100000 logs, elapsed time: 1.414089
70716 messages per second
100000 logs, elapsed time: 1.299567
76948 messages per second
100000 logs, elapsed time: 1.351263
74004 messages per second
100000 logs, elapsed time: 1.305027
76626 messages per second
100000 logs, elapsed time: 1.385899
72155 messages per second
100000 logs, elapsed time: 1.273577
78519 messages per second
        </pre>

        <h2 id="loggingLoopApp">loggingLoopApp - For debugging your logging system</h2>

        <p>In the process of development the requirement may arise
        to repeatedly send logs. Problems in communications, configuration
        file mis-configured, path problems in finding programs,
        downed boards, disconnected wiring, etc.</p>

        <p>The current implementation sends a logs once a second to
        logCollector. The log contents provide feedback of
        successful messaging and little else.</p>

        <p>To run this utility, use a two console: one for collecting
        the logs and one for loggingLoopApp. These terminals may
        be on the same node or on different nodes.</p>

        <p>Start the logCollector:<br/><code class=indent>logCollector --noisy</code></p>

        <p>On the other terminal start sending log messages:<br/><code>loggingLoopApp</code></p>

        <p>The terminal running loggingLoopApp will list something similar
        to:</p>

        <pre>
python loggingLoopApp.py
&gt;&gt;&gt; loggingLoopApp: pid 5413
loggingLoopApp: Client worker-RaspPi5413 started
Req #0 sent "request #0"
Req #1 sent "request #1"
Req #2 sent "request #2"
        </pre>

        <p>And the console where logCollector reports:</p>

        <pre>
2016-04-20T11:07:21.180	DEBUG	request #0,host=worker-RaspPi5413
2016-04-20T11:07:22.181	DEBUG	request #1,host=worker-RaspPi5413
2016-04-20T11:07:23.183	DEBUG	request #2,host=worker-RaspPi5413
        </pre>

        <p>The timestamp and worker ID will, of course, be different.</p>

        <p>Kill both the loggingLoopApp and logCollector with 
        ctrl-C in each terminal when finished.</p>

        <h2 id="listening">listening - Who is listening to that port?</h2>

        <p>Many times when developing software manual testing
        requires repeating the same process over and over.
        Due to various factors, it's easy to forget that
        the logCollector process is already running. It's certainly
        possible to forget the exact terminal logCollector runs
        on. listeningPort provides an answer to this situation.</p>

        <p>This utility must be run on the same node that logCollector
        runs.</p>

        <pre>
listening --help

Give a port that defaults to ZeroMQ port 5570,
list the processes that are listening to that port.

Use by:
  listening &lt;port&gt; [&lt;port1&gt; &lt;port2&gt; ...]
e.g.:
  listening 5570      # The ZeroMQ default port

Return codes:
  0 = Nobody listening to &lt;port&gt;
  127 = Error in port number input.
  &gt;0 = Number of processing listening to desired port.
        </pre>

        <ul>

            <li><code>--help</code> - Provide help message.</li>

            <li><code>port_number</code> - Use port_number
            for the logCollector.</li>

        </ul>

        <p>For convenience in debugging, <code>listening</code> can accept multiple
        ports on the same command line. This allows easier debugging
        when dealing with coordinating multiple processes. However,
        with multiple ports, the return code reflects only the state
        of the last port on the command line.</p>

        <pre>
        listening 5570 5571 5572 5573 5574
        Port 5570 : listening thru pid 7647 named /usr/lib/...
        Port 5571 : listening thru pid 7653 named /home/...
        Port 5573 : Nobody listening
        Port 5574 : Nobody listening
        </pre>

        <p>Start <code>listening</code> on some terminal. The response will be
        one of two possible output: (Of course, your pids will differ)</p>

        <pre>
# logCollector is running
listeningPort
Port 5570 : listening thru pid 5933 named /home/trailingdots/.local/bin/logCollector
        </pre>

        <pre>
# logCollector is not running, or it may be running on a different port.
listeningPort
Port 5570 : Nobody listening
        </pre>

        <p><b>Exercise:</b> Extend listeningPort to use a different node.
        Currently it detects port usage <i>only</i> on the same node as invoked.</p>

    <h1>Configuring a Node</h1>

    <p><code>logConfig.py</code> has setting that may need modification
    for each node. Using logging only on a single system requires no
    changes. This default setting works well.</p>


    <h1>Sample Code</h1>
    
    <p>In all the following example the logCollector must run. The
    logCollector process listens to the designated ZeroMQ port and
    save the incoming logs to a text file. This file may be read with
    any editor.</p>

    <p>Installation should have installed logCollector in a location
    within your $PATH environmental variable.</p>

    <h2 id="logCmdSample">Shell Logging: logCmd</h2>

    <p>Perhaps the simplest logging consists of a single command
    from a terminal. Shell commands provide a convenient way to
    record significant events of functioning systems. The code for
    <code>logCmd</code> also provides convenient tutorial for the
    use of the easy log messaging tools.</p>

    <p>The code that implements <code>logCmd</code>:</p>

    <pre>
import sys
import platform
import loggingClientTask

def main():
    client = loggingClientTask.LoggingClientClass(platform.node())
    client.start()

msg = ' '.join(sys.argv[1:])
client.info(msg)
    </pre>

    <p>The two lines of <code>main()</code> contain all needed information to
    send a log to the collector with a level of <code>INFO</code>.  These two
    lines create and start a logging process.</p>

    <p>The <code>platform()</code> informs LoggingClientClass that the
    payload portion of the log contains the name of the host. This example
    was run on my desktop.</p>

    <p>The last two lines space separate the command line arguments to create a
    string and sends this string to logCollector.</p>

    <h3>Exercise: Expand upon logCmd</h3>

    <p>Very simple code indeed. But this code could be a skeleton for
    much more complex applications.</p>

    <ul>

        <li>The original code uses the pre-configured logConfig
        for port addresses. Add a command line option that
        allows the user to specify a port. This will, of course,
        require adding a <code>usage()</code> routine
        as part a new <code>--help</code> option as well.</li>

        <li>The current code uses the <code>INFO</code> log level
        for recording logCmd entries. Allow the user to
        specify the level at run time. This permits a shell
        to log <code>ERROR</code>, <code>WARNING</code>, etc.</li>

        <li>The current logCmd works only on the node running
        logCollector. Extend this with a node argument to remote
        nodes and use shell commands to send logs.</p>

    </ul>

    <h1 id="tips">Tips for Great Logging</h1>

    <p>Even in our simple Raspberry Pi hydroponic system, a bit of thought
    placed into logging concepts will absolutely serve us well. Just
    logging for logging sakes does not provide an incentive to log. Having
    these logs provide a usable history and alarm system incentivises our
    logging structure.</p>

    <p>Think of logging as an integral part of our SCADA system.  SCADA =
    Supervisory Control and Data Acquisition.  SCADA gets used in remote
    monitoring and control systems that operates with coded signals over
    communications channels. <a href="https://en.wikipedia.org/wiki/SCADA">
    Wikipedia: SCADA</a></p>

    <p>Industrial uses of SCADA include electric power systems, water
    treatment plants, pipelines, chemical complexes, etc.</p>

    <p>Assume "hydro1" and "hydro2" are systems in a remote hydroponics
    garden with various measurement instrumentations. This remote system
    logs to a desktop inside the home.</p>

    <p>Log files should be readable by humans and computers alike.
    This implies a consistent format.</p>

    <p>Ease of searching. If you can't easily find the information 
    you need, it doesn't exist.</p>
    
    <p>Log exceptions properly. Stack traces should rarely get dumped
    into a log file. If you <i>realy</i> want to log exceptions,
    consider condensing them. (Note: I need to think about this
    more as exception tracing can greatly assist with production
    problems)</p>

    <p>Some examples of data that could appear in logs:</p>

    <ul>

         <li>A switch changing values ON/OFF.</li>

         <li>An instrument reporting temperature reading.</li>

         <li>A water level indicator reading too low or to high.</li>

         <li>A moisture level too low has triggered.</li>

         <li>A periodic report of temperature.</li>

    </ul>

    <p>All of the above and more could appear in your logs. The specific
    values in the logs must, of course, have appropriate values for that
    device.</p>

    <h2 id="logLevels">Production vs. Development Log Levels</h2>

    <p>When your system begin operating normally, the DEBUG level logs
    can flood your log system and become unwelcome guests.</p>

    <p>To change from a development level to production level,
    change <code>logConfig.LOG_LEVEL</code> to the appropriate
    level:</p>

    <pre>
    # In file logConfig.py:
    LOG_LEVEL = 'WARNING'
    </pre>

    <p>To include everything:</p>

    <pre>
    # In file logConfig.py:
    LOG_LEVEL = 'NOTSET'
    </pre>

    <p>In particular, the <code>CMD</code> level is above <code>WARNING</code>
    and before <code>ERROR</code> priority. This ordering ensures that
    normal operations can capture all <code>CMD</code> transactions.</p>

    <p>The <code>CMD</code> level create an out-of-band level for the
    python logging system. As such, this code maps the <code>CMD</code>
    level to the same level as <code>WARNING</code>.</p>

    <p><b>Excercise:</b> Add a conditional setting to this level
    setting based on an environmental variable. For example, an
    environmental variable "<code>PRODUCTION</code>" would indicate
    <code>DEBUG</code> and <code>INFO</code> levels get ignored.</p>

    <h2 id="UnknownState">The UNKNOWN state</h2>

    <p>The "<code>UNKNOWN</code>" value should apply to <i>every</i> device!  Devices
    can suffer all manner of reasons for refusing to work properly: water
    damage, physically damanged, wiring problems, aging, etc. A switch does
    not have a simple ON/OFF, but should actually use ON/OFF/UNKNOWN.</p>

     <h2>Notice your keywords</h2>

     <p>Keyword ease problems in interpreting log files. Log files can
     become easier to interpret if reasonable keywords and values
     describe each device. Humans have great linguistic abilities. Please
     allow your creations to communicate their reading
     with easily understood descriptions.</p>

     <p>Some suggestions for naming:</p>
        
        <table>
            <tr>
                <td><b>Keyword</b></td>
                <td><b>Meaning</b></td>
            </tr>
            <tr>
                <td>device</td>
                <td>Device name</td>
            </tr>
            <tr>
                <td>state</td>
                <td>Value for descrete devices: ON, OFF, UNKNOWN</td>
            </tr>
            <tr>
                <td>temp</td>
                <td>Temperature reading for analog temperature</td>
            </tr>
            <tr>
                <td>host</td>
                <td>Which system sent this data?</td>
            </tr>
            <tr>
                <td>cmd</td>
                <td>= req : A command request was sent. host=system performing request.<br/>
                =req : tag=xyz...&amp;host=central</td>
            </tr>
            <tr>
                <td>cmd</td>
                <td>=rep : A command reply indicates acknowledgement. host=sys performing command.
                  A reply sends the tag of the command. Optionally the entire
                  original command may populate the command.
                  cmd=rep&amp;tag=xyz&amp;host=hygro1</td>
             </tr>

         </table>

             <p>Devices in this example:</p>

             <ul>

                 <li>pumpWaterLevel = A water pump to maintain proper levels.</li>

                 <li>waterLevel = A floatation switch that detects water levels too high or too low.</li>

                 <li>tempIN, tempOUT = analog temperature measurements.</li>

             </ul>

        <h2>A Trivial Logging Example</h2>

        <p>Assume the following logs exist. (The comments are not part of the
        logs.)</p>

        <pre>
    # A periodic reading of water and temperature from several instruments
    2016-03-14T08:00:00.000    INFO    device=water01&amp;state=OFF&amp;host=hydro1
    2016-03-14T08:00:00.000    INFO    device=tempIN&amp;temp=72.3&amp;host=hydro1
    2016-03-14T08:00:00.000    INFO    device=tempOUT&amp;temp=69.2&amp;host=hydro1
    # Water level has gone too low
    2016-03-14T08:00:07.325    ERROR    device=water01&amp;state=LOW&amp;host=hydro1
    # Pump started to raise water level. A command was sent
    # pump01 request to start.
    2016-03-14T08:00:09.876    INFO    cmd=req&amp;tag=xyz&amp;device=pump01&amp;state=ON&amp;host=hydro1
    # Command started, remote sends reply. Note use of "tag"
    2016-03-14T08:00:09.876    INFO    cmd=rep&amp;tag=xyz&amp;host=hydro1
    # Water level back to normal and turn pump1 off.
    2016-03-14T08:05:05.325    INFO    device=water01&amp;state=OK&amp;host=hydro1
    # Command to turn pump01 off.
    2016-03-14T08:05:15.876    INFO    cmd=req&amp;tag=abc&amp;device=pump01&amp;state=OFF&amp;host=hydro1
    # Response: Pump01 starting to off state.
    2016-03-14T08:05:15.876    INFO    cmd=rep&amp;tag=abc&amp;host=hydro1
    # Periodic readings
    # One reading per device.
    2016-03-14T08:10:00.000    INFO    device=water01&amp;temp=71.2&amp;host=hydro1
    2016-03-14T08:10:00.000    INFO    device=pump01&amp;state=OFF&amp;host=hydro1
    2016-03-14T08:10:00.000    INFO    device=fan02&amp;state=OFF&amp;host=hydro1
    2016-03-14T08:10:00.000    INFO    device=temp04&amp;temp=71.1&amp;host=hydro1
    2016-03-14T08:10:00.000    INFO    device=temp03&amp;temp=70.5&amp;host=hydro1
        </pre>

        <p>Notice the uniformity of the above logs.</p>

        <ul>

            <li>One device per command. This eases analysis logic because
            parsing that one keyword allows easy processing of that one
            value.</li>

            <li>Each command request, "cmd=req&amp;tag=abc", has a tag. This
            ensures that the response with a command reply,
            "cmd=rep&amp;tab=abc", has been received by the remote.  The
            "cmd=rep&amp;abc" does <i>not</i> imply that the command was
            successfully completed, only that the remote received the
            command.</li>

            <li>The various instruments talk in their "native" representations.
            For instance, pumps belong to one of the ON, OFF or UNKNOWN states.
            A temperature measurement replies as "temp=65.2", etc. Your
            individual needs will, of course, vary.</li>

            <li>Macro commands could provide for a flurry of activity. An
            example of a macro command could be "circulate" wherein the pumps
            that circulate nutrient solutions activate and remain on during a
            pre-determined period.  These may orginate either in the remote or
            central control. Macros remain beyond the ken of loggers except to
            record the issuance of the command itself. The logger simply
            monitors and records activity.</li>

        </ul>


        <h1 id="TroubleShooting">TroubleShooting</h1>

        <h2>I get "permission denied" when I ...</h2>

        <p>Some part of your attempted operation tried to access
        areas protected by root access. Prefix you command with
        <code>sudo</code> and try again.</p>

        <h2>Undefined symbol: zmq_msg_gets</h2>

        <p>When starting logCollector, logCmd, ..., a bomb ending in
        <code>ImportError: ... undefined symbol: zmq_msg_gets</code>

        <p><code>sudo python</code> and <code>python</code> could execute
        different python versions due to $PATH. Run this:<p>

        <pre>
        python -c 'import zmq; print(zmq)'
        sudo python -c 'import zmq; print(zmq)'
        </pre>

        <p>If the <code>import zmq</code> fails, please refer to
        the web. This happened to me and it took a while to straighten
        this out. I wish I could tell you what to do in this case,
        but I'm not sure myself what fixed the problem. Yuck!</p>

        <h1>ISO8601 - What IS this?</h1>

        <p><a href="https://en.wikipedia.org/wiki/ISO_8601">ISO8601</a>
        refers to an international standard for representation of
        dates and times for the exchange of date and time related
        data. The ISO8601 standard provide an unambiguous and well-defined
        method of representing dates and time.</p>

        <p>For purposes of easy log messaging, each individual
        log entry gets tags with the ISO8601 date-time when
        logCollector receives the log.</p>
        
        <p>Systems using multiple nodes
        all have logs time stamped by the logCollector and <i>not</i>
        by the times on each node. This avoids time synchronization
        problems of keeping all nodes in time sync. This design
        element will not work for all systems, but for the targeted
        audience, Raspberry Pi/Arduino/..., this should not
        pose a problem.</p>

        <p>A deviation from the ISO8601 standard: easy log messaging
        uses local time as defined by the logCollector node. The
        standard demands UTC time. This was deemed confusing for purposes
        of this application.</p>

        <p>Easy log messaging uses local time, and when daylight
        savings time switches, the timestamps follow system time.
        This means an hour may be repeated or ignored depending
        upon the time of year.</p>

        <h2 id="CodingISO8601">Coding for ISO8601,....</h2>

        <p>Your code likely needs to records time one way or another.
        Coding for ISO8601 can confuse programs. Some examples can
        help.</p>

        <h3>An example using binary now to ISO8601 back to binary</h3>

        <pre>
import datetime
import time

"""
    Illustrate multiple conversions of timestamps

    Obtain the local time with millisecond precision.
    Convert local time to display format with local timezone.
    
    This time in display format gets placed into logger
    Now convert the display formatted time into seconds
    since the epoch.
""" 

# Read the current time. The time is in seconds since
# the start of the unix epoch, Jan 1, 1970.
unixNow = time.time()
print 'unix now:%f'% unixNow

# Create a python datetime from seconds.
now = datetime.datetime.fromtimestamp(unixNow)
print 'now=%s' % str(now)

# Format the python datetime into ISO8601 output
now8601 = now.strftime("%Y-%m-%dT%H:%M:%S.%f")
print 'now8601 = %s' % now8601

# Given a string, convert to a python tuple
nowTuple = datetime.datetime.strptime(now8601, "%Y-%m-%dT%H:%M:%S.%f")
print 'nowTuple:' + str(nowTuple)

# Convert from the tuple to seconds from the unix epoch.
secEpoch = time.mktime(nowTuple.timetuple()) + \
            1.0e-6*nowTuple.microsecond
print 'secEpoch: %s' % secEpoch
        </pre>

        <p>With output: (Obviously this is my current time)</p>

        <pre>
python iso.py
unix now:1461605636.495767
now=2016-04-25 10:33:56.495767
now8601 = 2016-04-25T10:33:56.495767
nowTuple:2016-04-25 10:33:56.495767
secEpoch: 1461605636.5
        </pre>

        <h3>To get a now ISO8601 local time string</h3>
        
        <p>A simple reading to get now in seconds from start
        of epoch to print this time in ISO8601 format.</p>

        <pre>
now = datetime.datetime.now()
print 'now=%s' % str(now)
nowStr = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")
print 'nowStr = %s' % nowStr

# To get now to a tuple:
nowTuple = datetime.datetime.strptime(nowStr, "%Y-%m-%d %H:%M:%S.%f")
print 'nowTuple:' + str(nowTuple)

msSinceEpoch = time.mktime(nowTuple.timetuple()) + \
                1.0e-6*nowTuple.microsecond
print 'msSinceEpoch:' + str(msSinceEpoch)
        </pre>

        <h3>Some linux date/time commands</h3>

        <p>These <a
            href="https://www.gnu.org/software/coreutils/manual/html_node/Examples-of-date.html">tips</a>
        could provide useful information at times.</p>

        <pre>
To convert a date string to the number of seconds since 
the start of the epoch use the "%s" format:
    date --date='1970-01-01 00:02:00 +0000' +%s
    120

If you do not specify time zone information in the date 
string, date uses your computer's idea of the time zone:
    # local time zone used
    date --date='1970-01-01 00:02:00' +%s
    18120

Also the --utc (-u) option:
    date --date='2000-01-01 UTC' +%s
    946684800

To convert such an unwieldy number of seconds back 
to a more readable form, use a command like this: 
    date -d @946684800 +"%F %T %z"
    1999-12-31 19:00:00 -0500

Often it is better to output UTC-relative date and time: 
    date -u -d '1970-01-01 946684800 seconds' +"%Y-%m-%d %T %z"
    2000-01-01 00:00:00 +0000
</pre>


        <h1>Original Ideas: ZeroMQ</h1>

        <p>This code uses ZeroMQ, also known as 0MQ or zmq, with python
        bindings, PyZMQ, to build this easy log messaging.  The <a
        href="http://zeromq.org/intro:read-the-manual">Asynchronous
    Client/Server Pattern</a> pattern forms the basic idea of this logger. The full
ZeroMQ pattern implements bidirectional messages while the easy log messaging
app uses the direction only from processes to the loggerControler.</p>

        <p>The easy log messaging app implementation derives from the
        Asynchronous Client/Server Pattern  in <a
        href="http://www.zeromq.com">ZeroMQ</a>, p. 111:<br/><center><img
        src="./images/AsyncClientServer1.svg"/></center></p>

        <p>This illustration uses three clients that access the RaspPi Logger.
        In practice the actual system may have one or more RaspPi systems
        sending logs to the RaspPi Logging system. The senders of the logs may
        also live either on the same system as the RaspPi loggers or on another
        system entirely. Remote systems may communicate with the RaspPi Logger
        through wireless transmission or an ethernet cable.  How is <i>that</i>
        for flexibility?</p>

        <h1>The Easy Messaging System - A Larger Vision</h1>

        <p>Easy log messaging just starts a suite of applications
        that provide an easy way to communicate with multiple
        nodes, be they Raspberry Pi, Arguino, Beagle Board
        and even desktop linux systems.</p>

        <p>I started with a logging system because I believe that
        basically <i>every</i> process envisioned needs a decent
        logging system to develop, debug and deploy multiple
        node systems.</p>

        <p>I used the excellent ZeroMQ libraries because ZeroMQ
        offers and ease of use not found in most messaging systems.
        ZeroMQ was wrapped in python because python is arguably the
        language of applications in this targeted market.</p>

        <p>Most importantly, I wanted relatively new developers
        from the Maker movement to have visions of connection multiple
        nodes to form new and creative methodologies to implement
        their own visions.</p>

        <p>To fully use the power of ZeroMQ for messaging requires
        two more basic modules: a name server and a control module.</p>

        <h2 id="EasyNameServer">Easy Name Server</h2>

        <p>Maintaining multiple nodes with coordinated communications
        can be a nightmare unless proper tools exist to support
        general node communications.</p>

        <p>A name server maintains a dictionary of names and port numbers.
        An external node knows it must communicate with a named node, e.g., "logCollector", so it queries the name server. The name server replies to the
        "logCollector" with the the proper port. This dictionary should
        persist between reboots.</p>

        <h2>Easy Control</h2>

        <p>Supplying commands to nodes must exist in some way. A unified
        command processing should ease application requirements for
        developers.<p>

        <p>In industry an example of this would be called "<a href="https://en.wikipedia.org/wiki/SCADA">SCADA</a>" - Supervisory Control And Data Acquisition.
        SCADA provides a system for remote monitoring and control.
        While SCADA systems can become quite elaborate, most of our needs
        could be quite modest.</p>

        <h2>A human Interface</h2>

        <p>One major issue with our control system is the human interface:
        "How do we display our data that allows a user to properly
        control our remote applications?"</p>

        <p>Since my vision for this tool encompasses all Maker style computer
        projects from light switches to robots to swarms of drones, a general
        human interface may be too general to be of much use. Perhaps multiple
        ideas require mulitple solutions. And most certainly some people have
        their own strong ideas; requirement ideas that are most valid
        indeed.</p>

        <p>A general human interface cannot be force fitted to this
        tool set. This problem may be addressed much futher down
        the line.</p>


        <h1 id="TODOList">TODO List</h1>

        <p>Non-trivial software always has something missing. The missing
        something differs from person to person, from user to user.
        An important item for one user exists as a distraction to another user.
        We all have our own view of reality.</p>

        <p>Overall, this logging systems seems to work well. Improvements,
        of couse, abound. No program can be considered dead until the
        last user has died. "Please close the door on the way out."</p>

        <p>Some items I consider important (in no particular order):</p>

        <ul>

            <li>Security. No security currently exists. As more and
            more systems get into the field, security becomes
            important. Do you want your home infested by a hacker?
            Definitely needed. Security should be a layer on top of
            the logging system.</li>

            <li>An easy way to change logging levels while the
            system is running. The <code>kill -USR1 &lt;pid&gr;</code>
            toggle the logCollector to echo on the console or not.
            Perhaps <code>USR2</code> could cycle around the 
            log levels?</li>

            <li>Docs for expanding log filtering.</li>

            <li>Speed increase. The current speed on my desktop
            of about 70,000 messages/second should handle all
            but the most demanding applications. Does this need attention?</li>

            <li>runTests.sh. This current functional test script detects only
            success or failure depending upon the return code of a run and does
            not examine output. This needs upgrading. However, currently, as od
            Apr, 2016, coverage is 92%. Create "golden" files? Scan output for
            special targets?</li>

            <li>More unit tests. Never have enough.</li>

            <li>Database for logs. Is this necessary? Too complicated?  An
            add-in component? Likely MongoDB because flexibility counts.</li>

            <li>Periodic log rollover. How large do logs get before
            the unwieldy size encumbers use? This obviously depends
            on what gets logged and user tolerance. Build a process
            that performs rollover periodically or by size?</li>

            <li>Easy date intervals. Filtering by "YESTERDAY", "TODAY",
            "LAST_WEEK", "LAST_HOUR", etc.</li>

            <li>Strange loss of "namespace": sometimes vars set in the
            __init__() method get "lost". This is particilarly true for ZeroMQ
            vars. For example, <code>self.context=zmq.Context()</code> set in
            init() but when used later in another class method has somehow
            become NULL!  This is not always true. The strange NULL appears is
            somewhat random. Likely a threading problem. Research has not
            yielded any viable insight. I am currently working around this with
            some kludgy looking code.</li>

            <li>Log exceptions properly. Stack traces should rarely get dumped
            into a log file. If you <i>realy</i> want to log exceptions,
            consider condensing them. I need to think about this
            more as exception tracing can greatly assist with production
            problems. Likely a special method to record exceptions
            with their stack traces.</li>

            <li>For logging on a single system, the payload identification
            of <code>host=hostname</code> does not work well enough to
            uniquely determine the source. What other scheme could
            allow exact identification?</li>

            <li>Give some thought to stack-trace handling. Systems
            inevitably crash and need sypport. The current incarnation
            does not handle multi-line messages intelligently.
            How should a stack-trace become encapsulated as a log?
            What is a good API?</li>
        </ul>


        <h1 id="Afterwords">Afterwords</h1>

        <p>I am reminded of the famous quote by <a href-"http://joearms.github.io/2013/05/31/a-week-with-elixir.html">Joe Armstrong</a>,
        creator of Erlang:</p>

        <center>
        <div style="width:400px;border:1px solid #cecece; padding:10px;">
            <blockquote><b>The Three Laws of Programming Language Design</b><br/><ul><li>What you get right, nobody mentions.</li><li>What you get wrong, people bitch about.</li><li>What is difficult to understand you have to explain to people over and over again.</li></ul></blockquote>
            <div class="credit" style="text-align:center;">
                <cite><a href="http://joearms.github.io/2013/05/31/a-week-with-elixir.html">Joe Armstrong on designing Erlang</a></cite>
            </div>
        </div>
        </center>

        <h1>Links and References</h1>

        <p><cite>ZeroMQ</cite> by Pieter Hintjens. Published by O'Reillly Media
        Inc, 2013.  <a href="http://www.zeromq.com">ZeroMQ</a>: this book
        provides a detailed description of ZeroMQ with many examples.  ZeroMQ
        offers a toolkit and not a framework. ZeroMQ has been used by <a
        href="http://zeromq.org/intro:read-the-manual"> many companies</a> as a
        basis for a wide variety of products. Highly recommended. Their website
        provides a plethora of ideas and practical advice.</p>

        <h1>License</h1>

        <p>In the spirit of the originators of ZeroMQ, the iMatix corporation,
        the following constitutes the license agreement:</p>

        <p>The project license is specified in COPYING and COPYING.LESSER.</p>

        <p>The easy log messaging app is free software; you can redistribute
        it and/or modify it under the terms of the GNU Lesser General Public
        License (LGPL) as published by the Free Software Foundation; either
        version 3 of the License, or (at your option) any later version.</p>

        <p>As a special exception, the Contributors give you permission to link
        this library with independent modules to produce an executable,
        regardless of the license terms of these independent modules, and to
        copy and distribute the resulting executable under terms of your
        choice, provided that you also meet, for each linked independent
        module, the terms and conditions of the license of that module. An
        independent module is a module which is not derived from or based on
        this library.  If you modify this library, you must extend this
        exception to your version of the library.</p>

        <p>The easy log messaging app is distributed in the hope that it will
        be useful, but WITHOUT ANY WARRANTY; without even the implied warranty
        of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
        Lesser General Public License for more details.</p>

    </body>

</html>
